{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-02T09:12:01.125121Z","iopub.execute_input":"2022-10-02T09:12:01.125554Z","iopub.status.idle":"2022-10-02T09:12:01.170570Z","shell.execute_reply.started":"2022-10-02T09:12:01.125461Z","shell.execute_reply":"2022-10-02T09:12:01.169643Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/data-process-2/train_cite_targets_sparse_13176.npz\n/kaggle/input/data-process-2/train_cite_targets_sparse_32606.npz\n/kaggle/input/data-process-2/train_cite_targets_sparse_31800.npz\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse_n32606.npz\n/kaggle/input/data-process-2/train_cite_inputs_important_sparse.npz\n/kaggle/input/data-process-2/train_cite_inputs_important_sparse_n13176.npz\n/kaggle/input/data-process-2/test_cite_inputs_important_sparse.npz\n/kaggle/input/data-process-2/__results__.html\n/kaggle/input/data-process-2/test_cite_inputs_filter_sparse.npz\n/kaggle/input/data-process-2/train_cite_inputs_important_sparse_13176.npz\n/kaggle/input/data-process-2/train_cite_inputs_important_sparsee_32606.npz\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse_13176.npz\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse_n13176.npz\n/kaggle/input/data-process-2/train_cite_targets_sparse_n31800.npz\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse_n31800.npz\n/kaggle/input/data-process-2/train_cite_inputs_important_sparse_n32606.npz\n/kaggle/input/data-process-2/__notebook__.ipynb\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse_31800.npz\n/kaggle/input/data-process-2/train_cite_targets_sparse_n32606.npz\n/kaggle/input/data-process-2/__output__.json\n/kaggle/input/data-process-2/train_cite_inputs_important_sparse_31800.npz\n/kaggle/input/data-process-2/train_cite_targets_sparse_n13176.npz\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse.npz\n/kaggle/input/data-process-2/train_cite_inputs_important_sparse_n31800.npz\n/kaggle/input/data-process-2/train_cite_inputs_filter_sparse_32606.npz\n/kaggle/input/data-process-2/custom.css\n/kaggle/input/data-process/multi_test_col_name.npy\n/kaggle/input/data-process/multi_test_row.npy\n/kaggle/input/data-process/test_cite_inputs_sparse.npz\n/kaggle/input/data-process/multi_y_mean.npy\n/kaggle/input/data-process/multi_test_col.npy\n/kaggle/input/data-process/__results__.html\n/kaggle/input/data-process/multi_y_std.npy\n/kaggle/input/data-process/multi_test_row_name.npy\n/kaggle/input/data-process/train_cite_inputs_sparse.npz\n/kaggle/input/data-process/train_cite_targets_sparse.npz\n/kaggle/input/data-process/__notebook__.ipynb\n/kaggle/input/data-process/__output__.json\n/kaggle/input/data-process/custom.css\n","output_type":"stream"}]},{"cell_type":"code","source":"# 导入库\nimport pandas as pd\nimport numpy as np\n# from tqdm import tqdm,tqdm_notebook \nfrom tqdm.notebook import tqdm,trange\nimport gc\nimport os\nimport sys\nfrom scipy import sparse\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nimport pickle\n!pip install tables","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:02.225699Z","iopub.execute_input":"2022-10-02T09:12:02.226419Z","iopub.status.idle":"2022-10-02T09:12:15.041080Z","shell.execute_reply.started":"2022-10-02T09:12:02.226381Z","shell.execute_reply":"2022-10-02T09:12:15.039842Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tables in /opt/conda/lib/python3.7/site-packages (3.7.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tables) (21.3)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from tables) (1.21.6)\nRequirement already satisfied: numexpr>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tables) (2.8.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tables) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# from sqrt4kaido\nclass NegativeCorrLoss(torch.nn.Module):\n    \"\"\"Negative correlation loss function for Keras\n\n    Precondition:\n    y_true.mean(axis=1) == 0\n    y_true.std(axis=1) == 1\n\n    Returns:\n    -1 = perfect positive correlation\n    1 = totally negative correlation\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, preds, targets):\n\n        my = torch.mean(preds, dim=1)\n        my = torch.tile(torch.unsqueeze(my, dim=1), (1, targets.shape[1]))\n        ym = preds - my\n        r_num = torch.sum(torch.multiply(targets, ym), dim=1)\n        r_den = torch.sqrt(\n            torch.sum(torch.square(ym), dim=1) * float(targets.shape[-1])\n        )\n        r = torch.mean(r_num / r_den)\n        return -r\ndef correlation_score(y_true, y_pred):\n    \"\"\"Scores the predictions according to the competition rules. \n    \n    It is assumed that the predictions are not constant.\n    \n    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n    if type(y_true) == pd.DataFrame: y_true = y_true.values\n    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n    corrsum = 0\n    for i in range(len(y_true)):\n        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n    return corrsum / len(y_true)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:15.044762Z","iopub.execute_input":"2022-10-02T09:12:15.045496Z","iopub.status.idle":"2022-10-02T09:12:15.056416Z","shell.execute_reply.started":"2022-10-02T09:12:15.045461Z","shell.execute_reply":"2022-10-02T09:12:15.055279Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class DatasetSparse_cite(torch.utils.data.Dataset):\n    def __init__(self,data_fil_x_path,data_imp_x_path,data_y_path):\n        \"\"\"\n        Params:\n        data_fil_x_path : 筛选后的特征\n        data_imp_x_path : 重要特征\n        data_y_path     : 数据标签\n        \"\"\"\n        self.x_fil = sparse.load_npz(data_fil_x_path) \n        self.x_imp = sparse.load_npz(data_imp_x_path) \n        Y = sparse.load_npz(data_y_path).toarray()\n        Y -= Y.mean(axis=1).reshape(-1, 1)\n        Y /= Y.std(axis=1).reshape(-1, 1)\n        self.y = Y\n        self.number = self.x_fil.shape[0]\n        assert self.x_fil.shape[0] == self.x_imp.shape[0]\n        print('Finish loading data into memory')\n        \n    def __len__(self):\n        return self.number\n\n    def __getitem__(self,idx):\n        return self.x_fil[idx],self.x_imp[idx],self.y[idx]\n    \ndef collate_fn_sparse_cite(batch):\n    \"\"\"\n    \"\"\"\n    index,feats,values,labels,feats_imp = [],[],[],[],[]\n    for idx, (feature,feature_imp,label) in enumerate(batch):\n        index+=[idx]*len(feature.indices)\n        feats+=feature.indices.tolist()\n        values+=feature.data.tolist()\n        labels+=[label.tolist()]\n        feats_imp+=[feature_imp.toarray().reshape(-1).tolist()]\n    return torch.tensor(index), torch.tensor(feats), torch.tensor(values),torch.tensor(feats_imp),torch.tensor(labels)\n\n\ndef collate_fn_sparse_pred(batch):\n    \"\"\"\n    \"\"\"\n    index,feats,values,feats_imp = [],[],[],[]\n    for idx, feature in enumerate(batch):\n        index+=[idx]*len(feature.indices)\n        feats+=feature.indices.tolist()\n        values+=feature.data.tolist()\n        feats_imp+=[feature_imp.toarray().reshape(-1).tolist()]\n    return torch.tensor(index), torch.tensor(feats), torch.tensor(values),torch.tensor(feats_imp)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:15.057667Z","iopub.execute_input":"2022-10-02T09:12:15.058454Z","iopub.status.idle":"2022-10-02T09:12:15.073113Z","shell.execute_reply.started":"2022-10-02T09:12:15.058414Z","shell.execute_reply":"2022-10-02T09:12:15.072099Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_fil_x_path = '../input/data-process-2/train_cite_inputs_filter_sparse_n13176.npz'\ndata_imp_x_path = '../input/data-process-2/train_cite_inputs_important_sparse_n13176.npz'\ndata_y_path = '../input/data-process-2/train_cite_targets_sparse_n13176.npz'\ndata_fil_x_path_valid = '../input/data-process-2/train_cite_inputs_filter_sparse_13176.npz'\ndata_imp_x_path_valid = '../input/data-process-2/train_cite_inputs_important_sparse_13176.npz'\ndata_y_path_valid = '../input/data-process-2/train_cite_targets_sparse_13176.npz'\ntrain_set = DatasetSparse_cite(data_fil_x_path,data_imp_x_path,data_y_path)\ntrain_set_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, \n                                               num_workers=2,drop_last=False,pin_memory=False,collate_fn=collate_fn_sparse_cite)\n\nvalid_set = DatasetSparse_cite(data_fil_x_path_valid,data_imp_x_path_valid,data_y_path_valid)\nvalid_set_loader = torch.utils.data.DataLoader(valid_set, batch_size=64, shuffle=False, \n                                               num_workers=2,drop_last=False,pin_memory=False,collate_fn=collate_fn_sparse_cite)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:15.075722Z","iopub.execute_input":"2022-10-02T09:12:15.076083Z","iopub.status.idle":"2022-10-02T09:12:33.127119Z","shell.execute_reply.started":"2022-10-02T09:12:15.076049Z","shell.execute_reply":"2022-10-02T09:12:33.126068Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Finish loading data into memory\nFinish loading data into memory\n","output_type":"stream"}]},{"cell_type":"code","source":"# model define\nclass LinearReg(torch.nn.Module):\n    \n    def __init__(self, model_config):\n        \"\"\"\n        model_config : {'input_dim':20000,'device':'cpu','output':140,'layer_one':160,'fc_dims':[64,32]}\n        \"\"\"\n        super(LinearReg, self).__init__()\n\n        self.input_dim = model_config['input_dim']\n        self.input_dim_imp = model_config['input_dim_imp']  # add code\n        self.device = model_config['device']\n        self.output = model_config['output']\n        self.layer_one = model_config['layer_one']\n        self.fc_dims = model_config['fc_dims']\n        self.layer_one_lst = torch.nn.ParameterList()\n        \n        for index,i in enumerate(range(self.layer_one)):\n            self.layer_one_lst.append(torch.nn.Parameter(torch.zeros(1, 1,device=self.device)))\n            self.layer_one_lst.append(torch.nn.Parameter(torch.randn(self.input_dim, 1,device=self.device))) # 多1维，从 1 开始\n            torch.nn.init.xavier_uniform_(self.layer_one_lst[index * 2+1],gain=1)\n        \n        num_dim = self.layer_one\n        dim = self.layer_one\n        \n        self.mats = torch.nn.ParameterList()\n        for (index, fc_dim) in enumerate(self.fc_dims): # [10,5,1]\n            num_dim+=fc_dim\n            self.mats.append(torch.nn.Parameter(torch.randn(dim, fc_dim,device=self.device)))\n            self.mats.append(torch.nn.Parameter(torch.zeros(1, fc_dim,device=self.device))) # 可以多个 b\n            torch.nn.init.kaiming_uniform_(self.mats[index * 2], mode='fan_in', nonlinearity='relu')\n            dim = fc_dim\n        \n        num_dim+=self.input_dim_imp\n        self.output_w = torch.nn.Parameter(torch.randn(num_dim, self.output,device=self.device))\n        self.output_b = torch.nn.Parameter(torch.zeros(1, self.output,device=self.device)) # 可以多个 b\n        torch.nn.init.kaiming_uniform_(self.output_w, mode='fan_in', nonlinearity='relu')\n        \n        \n            \n            \n    def first_order(self, batch_size, index, values, bias, weights):\n        # type: (int, Tensor, Tensor, Tensor, Tensor) -> Tensor\n        size = batch_size # 64\n        srcs = weights.view(1, -1).mul(values.view(1, -1)).view(-1) # weight [1234,1] values[1234]  -> 1x1232 -> 1234\n        output = torch.zeros(size, dtype=torch.float32,device = self.device) # 64 \n        # output=output.to(device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n        output.scatter_add_(0, index, srcs) # dim,index,src  index 是1维   output[index[i]]+=srcs[i] 遍历 index 的 i index:[0,0,0,0,0,\\cdots,1,1,\\cdots,63,63] e.eg [1234] \n        first = output + bias # [64] # wx+b 一阶\n        return first\n    \n    \n    def higher_order(self, batch_size, index, mats):\n        for i in range(int(len(mats) / 2)):\n            output = torch.relu(output.matmul(mats[i * 2]) + mats[i * 2 + 1]) # 全连接层\n    \n\n    def forward(self, batch_size, index, feats, values, feats_imp):\n        # type: (int, Tensor, Tensor, Tensor, Tensor) -> Tensor\n        # batch_size : 64\n        ret_lst = []\n        for idx,i in enumerate(range(self.layer_one)):\n            batch_first = F.embedding(feats, self.layer_one_lst[idx * 2+1]) # weights [100000,1]  feats: [a_1,a_1,\\cdots,a_64,a_64] e.g [1234] batch_first: [1234,1]\n            ret_lst.append(self.first_order(batch_size, index, values, self.layer_one_lst[idx * 2],batch_first).view(-1,1)) # [64,1]\n        output = torch.relu(torch.concat(ret_lst,axis=1))    \n        \n        ret_lst_two = [output]\n        for i in range(int(len(self.mats) / 2)):\n            output = torch.relu(output.matmul(self.mats[i * 2]) + self.mats[i * 2 + 1])\n            ret_lst_two.append(output) # 全连接层\n\n        ret_lst_two.append(feats_imp) # add code 将 feats_imp 拼接到最后一层\n        \n        second = torch.relu(torch.concat(ret_lst_two,axis=1))   \n            \n        return second.matmul(self.output_w)+self.output_b\n\n    @torch.jit.export\n    def get_name(self):\n        return \"LinearReg\"","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:33.129654Z","iopub.execute_input":"2022-10-02T09:12:33.130459Z","iopub.status.idle":"2022-10-02T09:12:33.150336Z","shell.execute_reply.started":"2022-10-02T09:12:33.130419Z","shell.execute_reply":"2022-10-02T09:12:33.149330Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_config = {\n'input_dim':train_set.x_fil.shape[1]+1,\n'input_dim_imp':train_set.x_imp.shape[1],\n'device':torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ,\n'output':140, # cite:140 multi:23418\n'layer_one':512, # cite:160  multi:256\n'fc_dims':[128,32],\n'epochs' : 10\n}\nreg_model = LinearReg(model_config)\n# optimizer = torch.optim.SGD(reg_model.parameters(), lr=0.01, momentum=0.9, weight_decay=0) # 5e-4\noptimizer = torch.optim.Adam(reg_model.parameters(), lr=0.001) \nloss_fn = NegativeCorrLoss()","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:33.151750Z","iopub.execute_input":"2022-10-02T09:12:33.152206Z","iopub.status.idle":"2022-10-02T09:12:36.217158Z","shell.execute_reply.started":"2022-10-02T09:12:33.152170Z","shell.execute_reply":"2022-10-02T09:12:36.216183Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(model_config)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:36.218415Z","iopub.execute_input":"2022-10-02T09:12:36.218773Z","iopub.status.idle":"2022-10-02T09:12:36.225714Z","shell.execute_reply.started":"2022-10-02T09:12:36.218739Z","shell.execute_reply":"2022-10-02T09:12:36.224684Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'input_dim': 20857, 'input_dim_imp': 144, 'device': device(type='cuda', index=0), 'output': 140, 'layer_one': 512, 'fc_dims': [128, 32], 'epochs': 10}\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in trange(model_config['epochs'],desc = 'epochs'):\n    losses = []\n    train_batch_iterator = tqdm(train_set_loader, disable=False, file=sys.stdout)\n    valid_batch_iterator = tqdm(valid_set_loader, disable=False, file=sys.stdout)\n    # 训练阶段\n    for index,feats,values,feats_imp,labels in train_batch_iterator:\n        index = index.to(model_config['device'])\n        feats = feats.to(model_config['device'])\n        values = values.to(model_config['device'])\n        labels = labels.to(model_config['device'])\n        feats_imp = feats_imp.to(model_config['device'])\n\n        optimizer.zero_grad() \n        # batch_size 最后一个不一样\n        batch_size = len(labels)\n        outputs = reg_model(batch_size, index, feats, values, feats_imp)\n\n        loss  = loss_fn(outputs,labels)\n\n        loss.backward() \n        optimizer.step() \n\n        losses.append(loss.item())\n        train_batch_iterator.set_postfix(loss=loss.item()) # 实时 batch 的loss\n        \n    # 评估阶段\n    losses_valid = [] \n    with torch.no_grad():\n        for index,feats,values,feats_imp,labels in valid_batch_iterator:\n            index = index.to(model_config['device'])\n            feats = feats.to(model_config['device'])\n            values = values.to(model_config['device'])\n            labels = labels.to(model_config['device'])\n            feats_imp = feats_imp.to(model_config['device'])\n            \n            batch_size = len(labels)\n            outputs = reg_model(batch_size, index, feats, values, feats_imp)\n            \n            loss  = loss_fn(outputs,labels)\n\n            losses_valid.append(loss.item())\n            \n    print('epoch:{} \\n loss:{:.6} \\n  valid:{:.6}'.format(epoch,np.mean(losses),np.mean(losses_valid)))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-02T09:12:36.227041Z","iopub.execute_input":"2022-10-02T09:12:36.228028Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"epochs:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc5381769334468b9db2fd25ddb6ef12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/763 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6459395e574442b99ace7f93cb28443a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/347 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"590f88dcd739418b91af5575e1c4bf21"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
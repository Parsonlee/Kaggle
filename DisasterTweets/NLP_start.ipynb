{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from sklearn import feature_extraction, model_selection\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "train_df = pd.read_csv(path + 'train.csv')\n",
    "test_df = pd.read_csv(path + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's up man?\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['target'] == 0]['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['target'] == 1]['text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baidu: '"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "# test\n",
    "example = 'baidu: https://www.baidu.com'\n",
    "remove_url(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: remove_url(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real or Fake\n",
      "Kaggle \n",
      "getting started\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_tags(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "# test\n",
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\"\n",
    "print(remove_tags(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: remove_tags(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_tags(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omg another Earthquake '"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: remove_emoji(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rmoving numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/nlh_skfx3hl5v5ywv3xbbpzr0000gq/T/ipykernel_55871/4063489105.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_df['text'] = train_df['text'].str.replace('\\d+', '')\n",
      "/var/folders/52/nlh_skfx3hl5v5ywv3xbbpzr0000gq/T/ipykernel_55871/4063489105.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_df['text'] = test_df['text'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "train_df['text'] = train_df['text'].str.replace('\\d+', '')\n",
    "test_df['text'] = test_df['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im a king'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "example = 'I\\'m a *king'\n",
    "remove_punct(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: remove_punct(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing multiple spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].str.replace('   ', ' ')\n",
    "train_df['text']=train_df['text'].str.replace('     ', ' ')\n",
    "train_df['text']=train_df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\n",
    "train_df['text']=train_df['text'].str.replace('  ', ' ')\n",
    "train_df['text']=train_df['text'].str.replace('â€”', ' ')\n",
    "train_df['text']=train_df['text'].str.replace('â€“', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['text'].str.replace('   ', ' ')\n",
    "test_df['text']=test_df['text'].str.replace('     ', ' ')\n",
    "test_df['text']=test_df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\n",
    "test_df['text']=test_df['text'].str.replace('  ', ' ')\n",
    "test_df['text']=test_df['text'].str.replace('â€”', ' ')\n",
    "test_df['text']=test_df['text'].str.replace('â€“', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4)\n",
      "(6090, 5) (1523, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "# split the data into train and validation\n",
    "train, val = model_selection.train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 52)\n",
      "[[0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "eg_count_vectorizer = count_vectorizer.fit_transform(train_df['text'][0:5])\n",
    "\n",
    "print(eg_count_vectorizer[0].todense().shape)\n",
    "print(eg_count_vectorizer[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 14740), (1523, 6047), (3263, 6047))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = count_vectorizer.fit_transform(train['text'])\n",
    "val_data = count_vectorizer.fit_transform(val['text'])\n",
    "test_data = count_vectorizer.transform(test_df['text'])\n",
    "\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "8FAL0CxfkLwc"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = x\n",
    "        if y is not None:\n",
    "            self.y = y\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = MyDataset(train_data, train['target'])\n",
    "val_set = MyDataset(val_data, val['target'])\n",
    "# test_set = MyDataset(test_data, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "QcmbNXi7dftY"
   },
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        self.params = hparams\n",
    "        self.lr = self.params['lr']\n",
    "        self.batch_size = self.params['batch_size']\n",
    "        self.weight_decay = self.params['weight_decay']\n",
    "        self.train_acc = tm.Accuracy()\n",
    "        self.val_acc = tm.Accuracy()\n",
    "        \n",
    "        # define the model\n",
    "        self.l1 = nn.Linear(10, 96)\n",
    "        self.l2 = nn.Linear(96, 64)\n",
    "        self.l3 = nn.Linear(64, 48)\n",
    "        self.l4 = nn.Linear(48, 24)\n",
    "        self.l5 = nn.Linear(24, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = self.l5(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimzer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimzer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y.long().squeeze())\n",
    "        \n",
    "        self.train_acc(y_hat, y.long().squeeze())\n",
    "        self.log('step', self.trainer.current_epoch)\n",
    "        self.log('train_acc', self.train_acc)\n",
    "        self.log('loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        self.val_acc(y_hat, y.long().squeeze())\n",
    "        self.log('step', self.trainer.current_epoch)\n",
    "        self.log('val_acc', self.val_acc)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(training_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(test_set, batch_size=self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'lr': 1e-4,\n",
    "    'batch_size': 256,\n",
    "    'weight_decay': 1e-2\n",
    "}\n",
    "model = Classifier(hparams)\n",
    "trainer = pl.Trainer(max_epochs=50)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
